== PHD/HAWQ overview lab

This lab will go through a general overview of using PHD and HAWQ. +
For lab and demo purposes, we'll be using a single node Hadoop and HAWQ installation, these will scale horizontally as needed.

Requirements

- PHD+HAWQ SingleNode installation ( available at network.pivotal.io )

If you are working with the Amazon Image, you need to start PHD cluster using Ambari console at http://<IP>:8080 user password is admin/pivotal +

=== Starting PHD & HAWQ

Follow this http://phd-start.cfapps.io/start_phd.pdf[link] to start PHD via Ambari

=== Accessing Hadoop (HDFS)

open a terminal and login using .pem key
$ssh -l <.pem key> ec2-user@<IPADDRESS>

become gpadmin user 

$sudo su - gpadmin

from the command line run

$git clone https://github.com/kgshukla/BigDataRoadshow-labs /home/gpadmin/Desktop/BigDataRoadshow-labs

$hdfs dfs -ls /

for the labs we need to create a diretory to put data, check if it already exists

$hdfs dfs -ls /user/sample

if it exists, then run "$hdfs dfs -rm /user/sample/sample.csv" to delete the file.

if it does not exists, then run "$hdfs dfs -mkdir /user/sample"

verify that the directory was created

$hdfs dfs -ls /user/

=== Loading data into HDFS

$cd /home/gpadmin/Desktop/BigDataRoadshow-labs/labs/phd-hawq

$ls -l

you should a set of sql scripts and a sample.csv data file

now put a copy of the file into HDFS

$hdfs dfs -put sample.csv /user/sample/

check to see if the data was loaded

$hdfs dfs -ls /user/sample

$hdfs dfs -tail /user/sample/sample.csv

=== Using HAWQ

Looks at the structure in HDFS for HAWQ

$hdfs dfs -ls -R /hawq_data

==== Create external table

Create and external table definition that points through to the csv data in HDFS

----
$cd /home/gpadmin/Desktop/BigDataRoadshow-labs/labs/phd-hawq/
$psql -f 1_create_ext_table.sql
----

Things to notice about this create statement

----
CREATE EXTERNAL TABLE stage.car_sample_data_ext (
----

An external table creates the definition of a table for which data will be brought in from an outside source to populate. This data is requested from the source at run time, each time a query is executed against that external table.

----
LOCATION ('pxf://localhost:50070/user/sample/sample.csv?profile=HdfsTextSimple')
FORMAT 'CSV' (HEADER)
LOG ERRORS INTO stage.sample_err SEGMENT REJECT LIMIT 10;
----

The LOCATION clause tell the external table where to go get the data from. In the case the PXF process which reaches out to HDFS for the file requested. This is also a CSV file with a header row. The LOG ERRORS INTO clause puts any errors created by tuples that do not match the table definition into a table. SEGMENT REJECT LIMIT allows for any one HAWQ segment process to run into 10 errors before the query decides the data source is bad and aborts.

==== Create internal table

Create a table internal to HAWQ so we can pull the data directly into HAWQ for faster access

----
$psql -f 2_create_hawq_table.sql
----

==== Load HAWQ Table

Load the data from the external table into the internal table

----
$psql -f 3_load_hawq_table.sql
----

==== Execute queries against table

Execute a query against the external table as well as against the internal table

----
$psql -f 4_rpm_bands.sql
----

==== Pivoral Hadoop Administration, Configuration and Management

Pivotal Hadoop is monitored and managed by Ambari. You have already learned how to start PHD using Ambari in previous lab. Now you focus on following

1. administration, configuration management
2. service addition (like HBASE, Zookeeper) via UI.

Follow this http://phd-admin.cfapps.io/phd_administration.pdf[link] for the lab

