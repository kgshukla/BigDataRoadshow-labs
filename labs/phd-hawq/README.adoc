== PHD/HAWQ overview lab

This lab will go through a general overview of using PHD and HAWQ. +
For lab and demo purposes, we'll be using a single node Hadoop and HAWQ installation, these will scale horizontally as needed.

Requirements

- PHD+HAWQ SingleNode installation ( available at network.pivotal.io )

If you are working with the Amazon Image (AMI:ami-c42301ac) a remote desktop client that supports an RDP connection may be required. +

For Mac OSX CoRD is reccommended. CoRD is free to download and can be obtained from this site: http://cord.sourceforge.net/

For Windows the included Remote Desktop Connection can be used: http://windows.microsoft.com/en-ca/windows/connect-using-remote-desktop-connection#connect-using-remote-desktop-connection=windows-7

The username and password for the AMI image is gpadmin/bebold.

=== Starting PHD & HAWQ

as gpadmin

from the desktop click start_all.sh

or 

from command line run
bash ~/Desktop/start_all.sh

Wait for the cluster start process to finish
Once the cluster is up and running you should be able to query hdfs via the command line

=== Cleaning up HDFS

To make sure system is clean in order to walk through the excersices run
./pre_clean.sh

=== Accessing Hadoop (HDFS)

open a terminal become gpadmin user 

$sudo su - gpadmin

from the command line run

$git clone https://github.com/kgshukla/BigDataRoadshow-labs /home/gpadmin/Desktop/BigDataRoadshow-labs

$hdfs dfs -ls /

for the labs we need to create a diretory to put data, check if it already exisits

$hdfs dfs -ls /user/sample

if it exists, then no need to create directory again. Go to "Using HAWQ" sub section.

$hdfs dfs -mkdir /user/sample

verify that the directory was created

$hdfs dfs -ls /user/

=== Loading data into HDFS

$cd /home/gpadmin/Labs/PHDHAWQ/

$ls -l

you should a set of sql scripts and a sample.csv data file

now put a copy of the file into HDFS

$hdfs dfs -put sample.csv /user/sample/

check to see if the data was loaded

$hdfs dfs -ls /user/sample

$hdfs dfs -tail /user/sample/sample.csv

=== Using HAWQ

Looks at the structure in HDFS for HAWQ

$hdfs dfs -ls -R /hawq_data

==== Create external table

Create and external table definition that points through to the csv data in HDFS

----
$cd /home/gpadmin/Desktop/BigDataRoadshow-labs/labs/phd-hawq/
$psql -f 1_create_ext_table.sql
----

Things to notice about this create statement

----
CREATE EXTERNAL TABLE stage.car_sample_data_ext (
----

An external table creates the definition of a table for which data will be brought in from an outside source to populate. This data is requested from the source at run time, each time a query is executed against that external table.

----
LOCATION ('pxf://localhost:50070/user/sample/sample.csv?profile=HdfsTextSimple')
FORMAT 'CSV' (HEADER)
LOG ERRORS INTO stage.sample_err SEGMENT REJECT LIMIT 10;
----

The LOCATION clause tell the external table where to go get the data from. In the case the PXF process which reaches out to HDFS for the file requested. This is also a CSV file with a header row. The LOG ERRORS INTO clause puts any errors created by tuples that do not match the table definition into a table. SEGMENT REJECT LIMIT allows for any one HAWQ segment process to run into 10 errors before the query decides the data source is bad and aborts.

==== Create internal table

Create a table internal to HAWQ so we can pull the data directly into HAWQ for faster access

----
$psql -f 2_create_hawq_table.sql
----

==== Load HAWQ Table

Load the data from the external table into the internal table

----
$psql -f 3_load_hawq_table.sql
----

==== Execute queries against table

Execute a query against the external table as well as against the internal table

----
$psql -f 4_rpm_bands.sql
----

