== Data Streaming with Spring XD 

This lab will go through a general overview using Spring XD. +
For lab and demo purposes, we'll be using a single node system, although Spring XD will scale horizontally as needed.

Pre-Requisites

- http://projects.spring.io/spring-xd/[Spring XD] v1.1.0+ installed 
- http://pivotal.io/big-data/pivotal-gemfire[GemFire] v8.1+ installed
- http://pivotal.io/big-data/pivotal-hd[PHD+Hawq] v2.1+  installed

=== Starting Pivotal Hadoop via Ambari
Follow the instructions on this http://phd-start.cfapps.io/start_phd.pdf[website] to start Pivotal Hadoop. It takes some time to start so make sure you start early.

Ask instructor for IPADDRESS.

=== Starting Spring XD
Make sure you stop SpringXD service via Ambari before continuing this lab.

Login to your assigned machine using the following (terminal for mac/linux and putty for windows). Ask instructor for .pem or .ppk file.

[source,bash]
----
$ssh -i  <.pem key> ec2-user@<IP-address>
----

Start a single node instance of Spring XD as gpadmin user, typing on a terminal:

[source,bash]
----
$ git clone https://github.com/kgshukla/BigDataRoadshow-labs /home/ec2-user/Desktop/BigDataRoadshow-labs
$ export JAVA_OPTS="-XX:PermSize=512m"
$ /opt/pivotal/spring-xd-1.1.2/xd/bin/xd-singlenode

 _____                           __   _______
/  ___|          (-)             \ \ / /  _  \
\ `--. _ __  _ __ _ _ __   __ _   \ V /| | | |
 `--. \ '_ \| '__| | '_ \ / _` |  / ^ \| | | |
/\__/ / |_) | |  | | | | | (_| | / / \ \ |/ /
\____/| .__/|_|  |_|_| |_|\__, | \/   \/___/
      | |                  __/ |
      |_|                 |___/
1.1.0.RELEASE                    eXtreme Data


Started : SingleNodeApplication

(...)

1.1.0.RELEASE  INFO DeploymentSupervisor-0 server.ContainerListener - Container arrived: Container{name='9b736207-17df-4ba8-bfb7-8f68a14ab466', attributes={ip=192.168.1.2, host=Fredericos-Air, groups=, pid=9011, id=9b736207-17df-4ba8-bfb7-8f68a14ab466}}
1.1.0.RELEASE  INFO DeploymentSupervisor-0 server.ContainerListener - Scheduling deployments to new container(s) in 15000 ms
----
Wait for the server startup to be complete, it will take a few seconds. Remember that the command will not end and you need to open another terminal for other operations +
Once the server is up and running, let's connect to it from the XD Shell (from another ssh terminal):

[source,bash]
----
$ xd-shell

 _____                           __   _______
/  ___|          (-)             \ \ / /  _  \
\ `--. _ __  _ __ _ _ __   __ _   \ V /| | | |
 `--. \ '_ \| '__| | '_ \ / _` |  / ^ \| | | |
/\__/ / |_) | |  | | | | | (_| | / / \ \ |/ /
\____/| .__/|_|  |_|_| |_|\__, | \/   \/___/
      | |                  __/ |
      |_|                 |___/
eXtreme Data
1.1.0.RELEASE | Admin Server Target: http://localhost:9393
Welcome to the Spring XD shell. For assistance hit TAB or type "help".
xd:>
----

As the server is running on the same machine, the XD Shell automatically recognized it and connected. +
We're ready to create our first streams.

First, let's check no stream is currently running:

[source,bash]
----
xd:> stream list
  Stream Name  Stream Definition  Status
  -----------  -----------------  ------
----
The list should be empty, meaning no streams currently exist.

=== Creating your first stream

Let's start creating a few very simple data streams that will simply read data from different sources and directly output to select destinations. 

* Reading the system's time and outputing to the log:

On the XD shell window, type:

[source,bash]
----
xd:> stream create stream1 --definition "time | log" --deploy
Created and deployed new stream 'stream1'
----
Check the logs on the window where you started __xd-singlenode__. It should be outputing time ticks like this. +
[source,bash]
----
2015-08-02 00:38:26,786 1.1.2.RELEASE  INFO task-scheduler-1 sink.stream1 - 2015-08-02 00:38:26
2015-08-02 00:38:27,788 1.1.2.RELEASE  INFO task-scheduler-2 sink.stream1 - 2015-08-02 00:38:27
2015-08-02 00:38:28,790 1.1.2.RELEASE  INFO task-scheduler-1 sink.stream1 - 2015-08-02 00:38:28
2015-08-02 00:38:29,791 1.1.2.RELEASE  INFO task-scheduler-3 sink.stream1 - 2015-08-02 00:38:29
2015-08-02 00:38:30,792 1.1.2.RELEASE  INFO task-scheduler-2 sink.stream1 - 2015-08-02 00:38:30
2015-08-02 00:38:31,793 1.1.2.RELEASE  INFO task-scheduler-4 sink.stream1 - 2015-08-02 00:38:31
----

Now the _stream list_ command should also identify the stream we created and we should be able to destroy it when desired:

[source,bash]
----
xd:>stream list
  Stream Name  Stream Definition  Status
  -----------  -----------------  --------
  stream1      time|log           deployed

xd:>stream destroy stream1
Destroyed stream 'stream1'

----
 
=== Sinking into the file system

 Now let's change it a little in order to output to a file instead.
 
* Reading the system's time and outputing to a file:
 
[source,bash]
----
xd:>stream create streamTimeFile --definition "time | file --name=mystream --dir=/tmp" --deploy
Created and deployed new stream 'streamTimeFile'
----

Check the log file on a new ssh terminal window and see the time is now being output there: +
 +
[source,bash]
----
$ tail -f /tmp/mystream.out
2015-03-13 23:38:19
2015-03-13 23:38:20

----

=== Listening HTTP port and outputing to a file 

As the next step on this exercise, let's change the _source_ to be something more useful than "time"  
If we specify _http_, XD will automatically start listening to HTTP connections on a port, where we can submit our data to:
 +
[source,bash]
----
xd:>stream create httptofile --definition "http --port=9020 | file --name=fromhttp --dir=/tmp" --deploy
Created and deployed new stream 'httptofile'
----

Now we'll use XD-shell itself to send a JSON object to that HTTP listener:
 +
[source,bash]
----
xd:> http post --target 'http://localhost:9020' --data '{Values : [{"X":0,"Y":1,"Z":0,"key":0},{"X":1,"Y":0,"Z":0,"key":1}]}'

> POST (text/plain;Charset=UTF-8) http://localhost:9020 {Values : [{"X":0,"Y":1,"Z":0,"key":0},{"X":1,"Y":0,"Z":0,"key":1}]}
> 200 OK
----

As expected, that data should be now in the /tmp/fromhttp.out file, as specified:
 +
[source,bash]
----
$ cat /tmp/fromhttp.out 
{Values : [{"X":0,"Y":1,"Z":0,"key":0},{"X":1,"Y":0,"Z":0,"key":1}]}
----
 
=== Extracting and transforming JSON data 
 
As a next step, we'll see how XD can be used to easily apply built-in transformations, like extracting specific fields from JSON requests on a data stream. +
Deploy the following stream:
 +
[source,bash]
----
xd:>stream create transform --definition "http --port=9030 | splitter --expression=#jsonPath(payload,'$.Values') | file --name=transform --dir=/tmp" --deploy
Created and deployed new stream 'transform'
----

Let's send the exact same data to this new stream:
 +
[source,bash]
----
xd:>http post --target 'http://localhost:9030' --data '{Values : [{"X":0,"Y":1,"Z":0,"key":0},{"X":1,"Y":0,"Z":0,"key":1}]}'
> POST (text/plain;Charset=UTF-8) http://localhost:9030 {Values : [{"X":0,"Y":1,"Z":0,"key":0},{"X":1,"Y":0,"Z":0,"key":1}]}
> 200 OK
----

The result, as output on the file specified, is the each value extracted as expected. 
 +
[source,bash]
----
$ cat /tmp/transform.out 
{X=0, Y=1, Z=0, key=0}
{X=1, Y=0, Z=0, key=1}
----
Each value of our JSON object array was extracted as a separate line by the _splitter_ module.
 
Next, we'll add an additional filter to the same definition, extracting only the lines where _Y_ has the value _0_
 +
[source,bash]
----
xd:>stream create transform2 --definition "http --port=9040 | splitter --expression=#jsonPath(payload,'$.Values') | filter --expression=#jsonPath(payload,'$.Y').equals(0) | file --name=transform2 --dir=/tmp" --deploy
Created and deployed new stream 'transform2'
----
 
Sending the exact same data as input, we should only see as output the line with the value specified on the filtering module:
 +
[source,bash]
----
 xd:>http post --target 'http://localhost:9040' --data '{Values : [{"X":0,"Y":1,"Z":0,"key":0},{"X":1,"Y":0,"Z":0,"key":1}]}'
> POST (text/plain;Charset=UTF-8) http://localhost:9040 {Values : [{"X":0,"Y":1,"Z":0,"key":0},{"X":1,"Y":0,"Z":0,"key":1}]}
> 200 OK
----
Checking the output..
[source,bash]
----
$ cat /tmp/transform2.out 
{X=1, Y=0, Z=0, key=1}
----

 Cleaning up everything for the next exercise:
[source,bash]
----
xd:>stream all destroy 
Really destroy all streams? [y, n]: y
Destroyed all streams
----
 
=== Applying an additional transformation and sinking into HDFS 

First we need to make sure Pivotal HD is up and running. You must have already started via Ambari in the first section.
If you're not sure, just check hadoop command
 +
[source,bash]
----
$ hdfs dfs -ls /

Found 11 items
drwxrwxrwx   - yarn    hadoop           0 2015-05-25 12:13 /app-logs
drwxr-xr-x   - hdfs    hdfs             0 2015-05-25 12:11 /apps
drwxrwxrwx   - gpadmin gpadmin          0 2015-05-26 13:21 /hawq_data
drwxr-xr-x   - mapred  hdfs             0 2015-05-25 12:09 /mapred
drwxr-xr-x   - hdfs    hdfs             0 2015-05-25 12:09 /mr-history
drwxr-xr-x   - hdfs    hdfs             0 2015-05-25 12:10 /phd
drwxrwxrwx   - hdfs    hdfs             0 2015-06-22 01:11 /sample
drwxr-xr-x   - hdfs    hdfs             0 2015-05-25 12:10 /system
drwxrwxrwx   - hdfs    hdfs             0 2015-07-06 09:20 /tmp
drwxr-xr-x   - hdfs    hdfs             0 2015-06-27 11:28 /user
drwxrwxrwx   - gpadmin hdfs             0 2015-08-02 02:44 /xd
----
In case it's stopped, use Ambari console to start up. 

* Configuring Spring XD for HDFS access

To configure the HDFS Namenode connection within Spring XD just use the command +hadoop config+
 +
[source,bash]
----
xd:>hadoop config fs --namenode hdfs://localhost:8020
----
Check connectivity by listening existing files on HDFS:
 +
[source,bash]
----
xd:>hadoop fs ls /
Hadoop configuration changed, re-initializing shell...
Found 8 items
drwxr-xr-x   - hdfs    hadoop          0 2014-08-24 11:54 /apps
drwxr-xr-x   - gpadmin hadoop          0 2014-08-24 12:02 /hawq_data
drwxr-xr-x   - hdfs    hadoop          0 2014-08-24 11:56 /hive
drwxr-xr-x   - mapred  hadoop          0 2014-08-24 11:55 /mapred
drwxrwxrwx   - hdfs    hadoop          0 2014-08-24 11:55 /tmp
drwxrwxrwx   - hdfs    hadoop          0 2015-03-14 04:07 /user
drwxrwxrwx   - hdfs    hadoop          0 2015-03-17 08:08 /xd
drwxr-xr-x   - hdfs    hadoop          0 2014-08-24 11:56 /yarn
----
Note the created *xd* directory, where Spring XD will output the streams created.

* Listening HTTP port and outputing to HDFS

Let's modify the previous stream to output to HDFS instead. We could just output the stream as-is to HDFS, but we'll convert it to CSV in order to be able to read with HAWQ.

=== Creating a simple transformer in groovy

Create the file _transform.groovy_ in your lab directory.
[source,bash]
----
cd /home/ec2-user/Desktop/BigDataRoadshow-labs/labs
----
The file can be created with the following command: +
[source,groovy]
----
touch transform.groovy
----
Next open the file for editing in vi with the following command in the terminal. +
[source,groovy]
----
vi transform.groovy
----
Finally paste the groovy script below into the file. Ensure to click the Save icon before closing gedit. +
[source,groovy]
----
csv = payload.get('X')+','+payload.get('Y')+','+payload.get('Z')+","+payload.get('key')
return csv
----

That's it. We'll use that as a last step in our transformation, before sinking to HDFS.

From the XD shell window, type (make sure you have the right path for the _transform.groovy_ file):
 +
[source,bash]
----
xd:>stream create transformToHDFS --definition "http --port=9020 | splitter --expression=#jsonPath(payload,'$.Values') | filter --expression=#jsonPath(payload,'$.Y').equals(0) | transform --script='file:/home/ec2-user/Desktop/BigDataRoadshow-labs/labs/transform.groovy'| hdfs --directory=/xd --fileName=output " --deploy
Created and deployed new stream 'transformToHDFS'
----
Wait for deployment to complete in xd-singlenode terminal. After completion, send some data in...
 +
[source,bash]
----
xd:>http post --target 'http://localhost:9020' --data '{Values : [{"X":0,"Y":1,"Z":0,"key":0},{"X":1,"Y":0,"Z":0,"key":1}]}'
> POST (text/plain;Charset=UTF-8) http://localhost:9020 {Values : [{"X":0,"Y":1,"Z":0,"key":0},{"X":1,"Y":0,"Z":0,"key":1}]}
> 200 OK
----
Now let's check the HDFS to see if we have the right data output:
 +
[source,bash]
----
xd:>hadoop fs ls /xd
Found 1 items
-rw-r--r--   3 gpadmin hadoop          0 2015-03-22 03:47 /xd/output-0.txt.tmp
----

While the file is being written to it will have the tmp suffix. When the data written exceeds the rollover size (default 1GB) it will be renamed to remove the tmp suffix. 

When you undeploy a stream
 +
[source,bash]
----
xd:>stream undeploy --name transformToHDFS
----

and list the stream directory again, in use file suffix doesn’t exist anymore.
Alternatively, as we already mentioned, one can change the rollover size to a smaller value, although for performance eficiency in HDFS bigger files are preferred. +

 
[source,bash]
----
xd:>hadoop fs ls /xd
Found 3 items
drwxrwxrwx   - root    hadoop          0 2015-03-04 14:49 /xd/connected-car
-rw-r--r--   3 gpadmin hadoop         28 2015-03-22 04:00 /xd/output-0.txt
drwxrwxrwx   - gpadmin hadoop          0 2015-03-17 08:10 /xd/s1
----
Now you can list the contents of the file. +
 
[source,bash]
----
xd:>hadoop fs cat /xd/output-0.txt
1,0,0,1
----

=== Tapping and inserting into GemFire

Now we're already splitting, filtering, transforming the input and sinking into HDFS we'll create a parallel stream data delivers the data to GemFire so we can consume that in real-time.

But first, lets deploy the transformToHDFS stream so we have something to tap into.

----
xd:>stream deploy --name transformToHDFS
Deployed stream 'transformToHDFS'
----

Now we will start a small Gemfire cluster on this machine, starting with the Locator process. We will cover Gemfire locators and servers in another lab.

- Start the GemFire locator in a new shell terminal as ec2-user user

[source,bash]
----
$ gfsh
    _________________________     __
   / _____/ ______/ ______/ /____/ /
  / /  __/ /___  /_____  / _____  / 
 / /__/ / ____/  _____/ / /    / /  
/______/_/      /______/_/    /_/    v8.1.0

Monitor and Manage GemFire

gfsh>start locator --name=locator1 --J=-Dgemfire.http-service-port=7575
Starting a GemFire Locator in /home/ec2-user/BigDataRoadshow/locator1...
-
Locator in /home/ec2-user/BigDataRoadshow/locator1 on ip-172-31-18-42.ec2.internal[10334] as locator1 is currently online.
Process ID: 46044
Uptime: 16 seconds
GemFire Version: 8.1.0
Java Version: 1.7.0_45
Log File: /home/ec2-user/BigDataRoadshow/locator1/locator1.log
JVM Arguments: -Dgemfire.enable-cluster-configuration=true -Dgemfire.load-cluster-configuration-from-dir=false -Dgemfire.http-service-port=7575 -Dgemfire.launcher.registerSignalHandlers=true -Djava.awt.headless=true -Dsun.rmi.dgc.server.gcInterval=9223372036854775806
Class-Path: /opt/pivotal/gemfire/Pivotal_GemFire_810/lib/gemfire.jar:/opt/pivotal/gemfire/Pivotal_GemFire_810/lib/locator-dependencies.jar

Successfully connected to: [host=ip-172-31-18-42.ec2.internal, port=1099]

Cluster configuration service is up and running.
----

- Start a GemFire server
[source,bash]
----
gfsh>start server --name=server1 --J=-Dgemfire.start-dev-rest-api=true --J=-Dgemfire.http-service-port=7676
Starting a GemFire Server in /home/gpadmin/Desktop/server1...
.............
Server in /home/ec2-user/Desktop/server1 on ip-172-31-44-219.ec2.internal[40404] as server1 is currently online.
Process ID: 16646
Uptime: 6 seconds
GemFire Version: 8.1.0
Java Version: 1.7.0_45
Log File: /home/ec2-user/Desktop/server1/server1.log
JVM Arguments: -Dgemfire.default.locators=172.31.44.219[10334] -Dgemfire.use-cluster-configuration=true -Dgemfire.start-dev-rest-api=true -Dgemfire.http-service-port=7676 -XX:OnOutOfMemoryError=kill -KILL %p -Dgemfire.launcher.registerSignalHandlers=true -Djava.awt.headless=true -Dsun.rmi.dgc.server.gcInterval=9223372036854775806
Class-Path: /opt/pivotal/gemfire/Pivotal_GemFire_810/lib/gemfire.jar:/opt/pivotal/gemfire/Pivotal_GemFire_810/lib/server-dependencies.jar
----
 - Create a GemFire region for storing the data sent from Spring XD

[source,bash]
----
gfsh>create region --name=ValuesFromXD --type=REPLICATE
Member  | Status
-
server1 | Region "/ValuesFromXD" created on "server1"
----
 - Create the Spring XD tap:
 
We'll create our tap from the _"filter"_ step on the _"transformToHDFS"_ stream, so GemFire will receive the data already filtered but still in JSon format.
On the XD shell, create the tap below:
 +
[source,bash]
----
xd:>stream create gemfireTap --definition "tap:stream:transformToHDFS.filter > object-to-json | gemfire-json-server --useLocator=true --host=localhost --port=10334 --regionName=ValuesFromXD --keyExpression=payload.getField('key')" --deploy
Created and deployed new stream 'gemfireTap'
----

Wait for deployment to complete in xd-singlenode terminal. After completion, send some data to our original stream again.

[source,bash]
----
xd:>http post --target 'http://localhost:9020' --contentType application/json --data '{Values : [{"X":0,"Y":1,"Z":0,"key":0},{"X":1,"Y":0,"Z":0,"key":1}]}'
> POST (application/json;charset=UTF-8) http://localhost:9020 {Values : [{"X":0,"Y":1,"Z":0,"key":0},{"X":1,"Y":0,"Z":0,"key":1}]}
> 200 OK
----

Now the data should not only be sinked as CSV into HDFS, but also be available in JSon format at GemFire.
Let's confirm GemFire has received the data we've submitted:

----
gfsh>describe region --name=/ValuesFromXD
Name            : ValuesFromXD
Data Policy     : replicate
Hosting Members : server1

Non-Default Attributes Shared By Hosting Members  

 Type  | Name | Value
------ | ---- | -----
Region | size | 1
----

As we can see, there's one value in, which we can easily verify using the GemFire's REST API:
link:http://<IP-ADDRESS>:7676/gemfire-api/v1/ValuesFromXD[http://<IP-ADDRESS>:7676/gemfire-api/v1/ValuesFromXD]

----
{
  "ValuesFromXD" : [ {
    "X" : 1,
    "Y" : 0,
    "Z" : 0,
    "key" : 1
  } ]
}
----
Before moving on to HAWQ, lets shut down Gemfire. Type following in gfsh terminal.

----
gfsh>shutdown --include-locators=true
As a lot of data in memory will be lost, including possibly events in queues, do you really want to shutdown the entire distributed system? (Y/n): Y
Shutdown is triggered

gfsh>
No longer connected to ip-172-31-44-219.ec2.internal[1099].
----

=== Quering from HAWQ

We will now create a table that can query the data that is being written to HDFS. +
Open a new terminal session and type in the command:
[source, bash]
----
$sudo su - gpadmin
$psql
----

With in the psql terminal session, paste the follwing SQL. +
[source,sql]
----
CREATE EXTERNAL TABLE test (
  X int,
  Y int,
  Z int,
  key int )
LOCATION
('pxf://localhost:50070/xd/output-*.txt?profile=HdfsTextSimple')
FORMAT 'CSV'
LOG ERRORS INTO test_err SEGMENT REJECT LIMIT 10;


Now execute the following query 

gpadmin=# select * from test;

 x | y | z | key 
---+---+---+-----
 1 | 0 | 0 |   1
(1 row)

Exit the prompt by typing \q-
gpadmin=# \q

----

image::architecture.png[]


